{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Mask Detection Classsifier\n\nDuring the COVID-19 pandemic, we have come to realize the importance of using face masks to limit the spread of the virus.\n\nBeing able to identify places with high concentration of people not wearing a mask  in advance, will allow hospitals and governments to prepare and allocate resources ahead of time.\n\nThis project aims to build a model which takes images of crowds and identifies whether or not they wear a mask correctly.\n\nThe input data for this project is a set of images (853) along with an annotations file. Each image may include more than one person, and the annotations file contains general information about the image, as well as the coordinates of each face and its label.\n\nThis data includes 3 labels - With mask; Without mask; Mask worn incorrectly.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # image plotting\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:56:59.439619Z","iopub.execute_input":"2022-08-11T03:56:59.440205Z","iopub.status.idle":"2022-08-11T03:56:59.467407Z","shell.execute_reply.started":"2022-08-11T03:56:59.440126Z","shell.execute_reply":"2022-08-11T03:56:59.466733Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"The input data includes two files - an image- which is available in the **images** folder, and annotations file which is available in the **annotations** folder. The name of each image and its corresponding annotation file is the same, except for the suffix.","metadata":{}},{"cell_type":"code","source":"input_data_path = '/kaggle/input/face-mask-detection/images'\nannotations_path = \"/kaggle/input/face-mask-detection/annotations\"\nimages = [*os.listdir(\"/kaggle/input/face-mask-detection/images\")]\noutput_data_path =  '.'\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:57:02.825303Z","iopub.execute_input":"2022-08-11T03:57:02.825612Z","iopub.status.idle":"2022-08-11T03:57:03.172439Z","shell.execute_reply.started":"2022-08-11T03:57:02.825589Z","shell.execute_reply":"2022-08-11T03:57:03.171727Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Read the annotations file into a Pandas DataFrame","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET # is used to parse an XML (inherently hierarchical) data format, which is the format of the annotations file\n\ndef parse_annotation(path):\n    tree = ET.parse(path)\n    root = tree.getroot()\n    constants = {}\n    objects = [child for child in root if child.tag == 'object']\n    for element in tree.iter():\n        if element.tag == 'filename':\n            constants['file'] = element.text[0:-4]\n        if element.tag == 'size':\n            for dim in list(element):\n                if dim.tag == 'width':\n                    constants['width'] = int(dim.text)\n                if dim.tag == 'height':\n                    constants['height'] = int(dim.text)\n                if dim.tag == 'depth':\n                    constants['depth'] = int(dim.text)\n    object_params = [parse_annotation_object(obj) for obj in objects]\n    #print(constants)\n    full_result = [merge(constants,ob) for ob in object_params]\n    return full_result   \n\n\ndef parse_annotation_object(annotation_object):\n    params = {}\n    for param in list(annotation_object):\n        if param.tag == 'name':\n            params['name'] = param.text\n        if param.tag == 'bndbox':\n            for coord in list(param):\n                if coord.tag == 'xmin':\n                    params['xmin'] = int(coord.text)              \n                if coord.tag == 'ymin':\n                    params['ymin'] = int(coord.text)\n                if coord.tag == 'xmax':\n                    params['xmax'] = int(coord.text)\n                if coord.tag == 'ymax':\n                    params['ymax'] = int(coord.text)\n            \n    return params       \n \ndef merge(dict1, dict2):\n    res = {**dict1, **dict2}\n    return res","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:57:14.939122Z","iopub.execute_input":"2022-08-11T03:57:14.939461Z","iopub.status.idle":"2022-08-11T03:57:14.950059Z","shell.execute_reply.started":"2022-08-11T03:57:14.939437Z","shell.execute_reply":"2022-08-11T03:57:14.949135Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import glob\ndataset = [parse_annotation(anno) for anno in glob.glob(annotations_path+\"/*.xml\") ]\n\n# Since the output of the parse_annotation function is a list of lists, we need to flatten the ctopped faces.\n# i.e make it a list of images instead of a list of lists.\nfull_dataset = sum(dataset, []) # \n#full_dataset\n\ndf = pd.DataFrame(full_dataset)\ndf.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:57:30.825245Z","iopub.execute_input":"2022-08-11T03:57:30.825564Z","iopub.status.idle":"2022-08-11T03:57:35.801229Z","shell.execute_reply.started":"2022-08-11T03:57:30.825541Z","shell.execute_reply":"2022-08-11T03:57:35.800067Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:57:39.448639Z","iopub.execute_input":"2022-08-11T03:57:39.448977Z","iopub.status.idle":"2022-08-11T03:57:39.466600Z","shell.execute_reply.started":"2022-08-11T03:57:39.448952Z","shell.execute_reply":"2022-08-11T03:57:39.465372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Omit one image from the dataset in order to use it at the end of the project for illustrating the results of the model","metadata":{}},{"cell_type":"code","source":"final_test_image = 'maksssksksss0' # chose the image\ndf_final_test = df.loc[df[\"file\"] == final_test_image] # create a separate dataframe which contain only the people in this specific image\nimages.remove(f'{final_test_image}.png') # remove the image from the full dataset\ndf = df.loc[df[\"file\"] != final_test_image] # remove the information of the image from the full dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:57:52.362953Z","iopub.execute_input":"2022-08-11T03:57:52.363253Z","iopub.status.idle":"2022-08-11T03:57:52.376215Z","shell.execute_reply.started":"2022-08-11T03:57:52.363231Z","shell.execute_reply":"2022-08-11T03:57:52.374666Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_final_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:58:18.215565Z","iopub.execute_input":"2022-08-11T03:58:18.216243Z","iopub.status.idle":"2022-08-11T03:58:18.227846Z","shell.execute_reply.started":"2022-08-11T03:58:18.216176Z","shell.execute_reply":"2022-08-11T03:58:18.226902Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Rename some columns\n","metadata":{}},{"cell_type":"code","source":"df.rename(columns = {'file':'file_name', 'name':'label'}, inplace = True)\ndf_final_test.rename(columns = {'file':'file_name', 'name':'label'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:58:24.328306Z","iopub.execute_input":"2022-08-11T03:58:24.328851Z","iopub.status.idle":"2022-08-11T03:58:24.335726Z","shell.execute_reply.started":"2022-08-11T03:58:24.328826Z","shell.execute_reply":"2022-08-11T03:58:24.334831Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Plot the label distribution in the data","metadata":{}},{"cell_type":"code","source":"df[\"label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:58:37.059294Z","iopub.execute_input":"2022-08-11T03:58:37.059620Z","iopub.status.idle":"2022-08-11T03:58:37.070503Z","shell.execute_reply.started":"2022-08-11T03:58:37.059597Z","shell.execute_reply":"2022-08-11T03:58:37.069376Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df[\"label\"].value_counts().plot(kind='barh')\nplt.xlabel('Count', fontsize = 10, fontweight = 'bold')\nplt.ylabel('Label', fontsize = 10, fontweight = 'bold')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:58:40.214210Z","iopub.execute_input":"2022-08-11T03:58:40.214524Z","iopub.status.idle":"2022-08-11T03:58:40.386479Z","shell.execute_reply.started":"2022-08-11T03:58:40.214501Z","shell.execute_reply":"2022-08-11T03:58:40.385155Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"As shown above, we have to deal with an unbalanced dataset. The appropriate solution for this kind of problem (where this distribution reflects the distribution in reality) is to split the data in a way that this distribution will be preserved in the train / test / validation sets. ","metadata":{}},{"cell_type":"markdown","source":"## Prepare the data\n\nThe next goal is to rearrange our data in new folders, according to their label. We first create an empty folder for each data set (train/ test/ validation) and for each of the 3 labels.","metadata":{}},{"cell_type":"code","source":"labels = df['label'].unique()\ndirectory = ['train', 'test', 'val']\noutput_data_path =  '.'\n\nimport os\nfor label in labels:\n    for d in directory:\n        path = os.path.join(output_data_path, d, label)\n        #print(path)\n        if not os.path.exists(path):\n            os.makedirs(path)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:58:45.457496Z","iopub.execute_input":"2022-08-11T03:58:45.457974Z","iopub.status.idle":"2022-08-11T03:58:45.463881Z","shell.execute_reply.started":"2022-08-11T03:58:45.457949Z","shell.execute_reply":"2022-08-11T03:58:45.462898Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Since each original image may contain multiple faces, we will need to crop all the faces from each image, based on the given coordinates, and copy them to the appropriate folder based on the label of each person. By that, we will recieve a dataset where each image consists of only one face + it's label. \n\nWe will divide these steps into multiple functions. The following function will crop the image based on the given coordinates. We added a small shift (10% of the face size ) in both X and Y directions in order to include more data in each image. ","metadata":{}},{"cell_type":"code","source":"from PIL import Image\ndef crop_img(image_path, x_min, y_min, x_max, y_max):\n    \n    '''\n     This function takes an image path + x and y coordinates of two opposite corners of the rectangle \n     and returns a cropped image\n    '''\n    x_shift = (x_max - x_min) * 0.1\n    y_shift = (y_max - y_min) * 0.1\n    img = Image.open(image_path)\n    cropped = img.crop((x_min - x_shift, y_min - y_shift, x_max + x_shift, y_max + y_shift))\n    return cropped","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:59:04.720813Z","iopub.execute_input":"2022-08-11T03:59:04.721145Z","iopub.status.idle":"2022-08-11T03:59:04.726837Z","shell.execute_reply.started":"2022-08-11T03:59:04.721119Z","shell.execute_reply":"2022-08-11T03:59:04.726212Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The next function extracts all the people from each image","metadata":{}},{"cell_type":"code","source":"def extract_faces(image_name, image_info):\n    \n    '''\n     This function takes an image name + dataframe with information about the image \n     and splits the image into all the different faces. image name contains the \n     upper-left coordinate of each face so we could distinguish it later\n    '''\n    faces = []\n    df_one_img = image_info[image_info['file_name'] == image_name[:-4]][['xmin', 'ymin', 'xmax', 'ymax', 'label']]\n    #print(df_one_img)\n    for row_num in range(len(df_one_img)):\n        x_min, y_min, x_max, y_max, label = df_one_img.iloc[row_num] \n        image_path = os.path.join(input_data_path, image_name)\n        faces.append((crop_img(image_path, x_min, y_min, x_max, y_max), label,f'{image_name[:-4]}_{(x_min, y_min)}'))\n    return faces","metadata":{"execution":{"iopub.status.busy":"2022-08-11T03:59:10.076973Z","iopub.execute_input":"2022-08-11T03:59:10.077300Z","iopub.status.idle":"2022-08-11T03:59:10.082940Z","shell.execute_reply.started":"2022-08-11T03:59:10.077275Z","shell.execute_reply":"2022-08-11T03:59:10.082370Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We now apply the `extract_faces` function on our data set","metadata":{}},{"cell_type":"code","source":"cropped_faces = [extract_faces(img, df) for img in images]","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:00:10.515109Z","iopub.execute_input":"2022-08-11T04:00:10.515467Z","iopub.status.idle":"2022-08-11T04:01:17.951559Z","shell.execute_reply.started":"2022-08-11T04:00:10.515444Z","shell.execute_reply":"2022-08-11T04:01:17.950585Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Since the output of the `extract_faces` function is a list of lists, we need to flatten the cropped faces. i.e make it a list of images instead of a list of lists.","metadata":{}},{"cell_type":"code","source":"#flat_cropped_faces = [item for sublist in cropped_faces for item in sublist]\nflat_cropped_faces = sum(cropped_faces, [])\n#flat_cropped_faces","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:01:22.399069Z","iopub.execute_input":"2022-08-11T04:01:22.399420Z","iopub.status.idle":"2022-08-11T04:01:22.415675Z","shell.execute_reply.started":"2022-08-11T04:01:22.399392Z","shell.execute_reply":"2022-08-11T04:01:22.414314Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Sort the images into their labels according to our 3 classes. ","metadata":{}},{"cell_type":"code","source":"with_mask = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == \"with_mask\"]\nmask_weared_incorrect = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == \"mask_weared_incorrect\"]\nwithout_mask = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == \"without_mask\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:01:25.317884Z","iopub.execute_input":"2022-08-11T04:01:25.318197Z","iopub.status.idle":"2022-08-11T04:01:25.326232Z","shell.execute_reply.started":"2022-08-11T04:01:25.318174Z","shell.execute_reply":"2022-08-11T04:01:25.325586Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Sanity check","metadata":{}},{"cell_type":"code","source":"print(f'num of images with mask: {len(with_mask)}')\nprint(f'num of images without mask: {len(without_mask)}')\nprint(f'num of images incorrect mask: {len(mask_weared_incorrect)}')\nprint(f'sum: {len(with_mask) + len(without_mask) + len(mask_weared_incorrect) }')","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:01:28.453399Z","iopub.execute_input":"2022-08-11T04:01:28.453846Z","iopub.status.idle":"2022-08-11T04:01:28.459006Z","shell.execute_reply.started":"2022-08-11T04:01:28.453822Z","shell.execute_reply":"2022-08-11T04:01:28.458094Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We now split the full data set into train (80%) and test (20%) sets. The test set is split again to test (30%) and validation (70%) sets. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_with_mask, test_with_mask = train_test_split(with_mask, test_size=0.20, random_state=42)\ntest_with_mask, val_with_mask = train_test_split(test_with_mask, test_size=0.7, random_state=42)\n\ntrain_mask_weared_incorrect, test_mask_weared_incorrect = train_test_split(mask_weared_incorrect, test_size=0.20, random_state=42)\ntest_mask_weared_incorrect, val_mask_weared_incorrect = train_test_split(test_mask_weared_incorrect, test_size=0.7, random_state=42)\n\ntrain_without_mask, test_without_mask = train_test_split(without_mask, test_size=0.20, random_state=42)\ntest_without_mask, val_without_mask = train_test_split(test_without_mask, test_size=0.7, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:01:36.216158Z","iopub.execute_input":"2022-08-11T04:01:36.216482Z","iopub.status.idle":"2022-08-11T04:01:36.677055Z","shell.execute_reply.started":"2022-08-11T04:01:36.216459Z","shell.execute_reply":"2022-08-11T04:01:36.676086Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def save_image(image, image_name, output_data_path,  dataset_type, label):\n    '''\n     This function takes an image name + a path of output folder\n     and saves image into the output folder\n    '''\n\n    output_path = os.path.join(output_data_path, dataset_type, label ,f'{image_name}.png')\n    image.save(output_path)   ","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:01:44.371696Z","iopub.execute_input":"2022-08-11T04:01:44.372449Z","iopub.status.idle":"2022-08-11T04:01:44.376270Z","shell.execute_reply.started":"2022-08-11T04:01:44.372422Z","shell.execute_reply":"2022-08-11T04:01:44.375646Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Save each image in the correct folder ","metadata":{}},{"cell_type":"code","source":"# Train set\n\n\nfor image, image_name in train_with_mask:\n    save_image(image, image_name, output_data_path, 'train', 'with_mask')\n\nfor image, image_name in train_mask_weared_incorrect:\n    save_image(image, image_name, output_data_path, 'train', 'mask_weared_incorrect')\n\nfor image, image_name in train_without_mask:\n    save_image(image, image_name, output_data_path, 'train', 'without_mask')\n        \n# Test set\n\nfor image, image_name in test_with_mask:\n    save_image(image, image_name, output_data_path, 'test', 'with_mask')\n\nfor image, image_name in test_mask_weared_incorrect:\n    save_image(image, image_name, output_data_path, 'test', 'mask_weared_incorrect')\n\nfor image, image_name in test_without_mask:\n    save_image(image, image_name, output_data_path, 'test', 'without_mask')\n    \n# Val set\n    \nfor image, image_name in val_with_mask:\n    save_image(image, image_name, output_data_path, 'val', 'with_mask')\n\nfor image, image_name in val_without_mask:\n    save_image(image, image_name, output_data_path, 'val', 'without_mask')\n\nfor image, image_name in val_mask_weared_incorrect:\n    save_image(image, image_name, output_data_path, 'val', 'mask_weared_incorrect')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:01:47.650059Z","iopub.execute_input":"2022-08-11T04:01:47.650871Z","iopub.status.idle":"2022-08-11T04:01:54.275631Z","shell.execute_reply.started":"2022-08-11T04:01:47.650827Z","shell.execute_reply":"2022-08-11T04:01:54.274594Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Model Building - CNN","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 16, kernel_size = 3,  padding='same', activation = 'relu', input_shape = (35,35,3)))\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Conv2D(filters = 32, kernel_size = 3,  padding='same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Conv2D(filters = 64, kernel_size = 3,  padding='same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(units = 500, activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units = 3, activation = 'softmax'))\n\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:00.641047Z","iopub.execute_input":"2022-08-11T04:02:00.641412Z","iopub.status.idle":"2022-08-11T04:02:06.170108Z","shell.execute_reply.started":"2022-08-11T04:02:00.641387Z","shell.execute_reply":"2022-08-11T04:02:06.169106Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the model architecture","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:14.368146Z","iopub.execute_input":"2022-08-11T04:02:14.368581Z","iopub.status.idle":"2022-08-11T04:02:15.436986Z","shell.execute_reply.started":"2022-08-11T04:02:14.368558Z","shell.execute_reply":"2022-08-11T04:02:15.436056Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"For convenience, we would like to define here some constant parameters  \n","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nepochs = 50","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:22.425159Z","iopub.execute_input":"2022-08-11T04:02:22.425481Z","iopub.status.idle":"2022-08-11T04:02:22.430822Z","shell.execute_reply.started":"2022-08-11T04:02:22.425456Z","shell.execute_reply":"2022-08-11T04:02:22.428885Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"To enhance our model performance, we would like to add more data to train on. We will use data augmantation to artificially create more data samples.\n\nTo do that, we will use the Keras `ImageDataGenerator`. This tool allows us to create new trainning images by manipulating the exiting ones (scaling, rotating, flipping, etc.). However, since not all manipulations make sense in the context of face image, i.e. flipping a face vertically is not a valid option for a person walking down the street , we will only use a small subset of the possible manipulations.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rescale=1.0 / 255, horizontal_flip=True, zoom_range=0.1, shear_range=0.2, width_shift_range=0.1,\n    height_shift_range=0.1, rotation_range=4, vertical_flip=False\n\n)\n\nval_datagen = ImageDataGenerator(\n    rescale=1.0 / 255\n)\n \n\ntrain_generator = datagen.flow_from_directory(\n    directory='/kaggle/working/train', \n    target_size = (35,35),\n    class_mode=\"categorical\", batch_size=batch_size, shuffle=True\n\n)\n\n# Validation data\nval_generator = val_datagen.flow_from_directory(\n    directory='/kaggle/working/val', \n    target_size = (35,35),\n    class_mode=\"categorical\", batch_size=batch_size, shuffle=True\n)\n\n# Test data\ntest_generator = val_datagen.flow_from_directory(\n    directory='/kaggle/working/test', \n    target_size = (35,35),\n    class_mode=\"categorical\", batch_size=batch_size, shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:28.131804Z","iopub.execute_input":"2022-08-11T04:02:28.132131Z","iopub.status.idle":"2022-08-11T04:02:28.858747Z","shell.execute_reply.started":"2022-08-11T04:02:28.132096Z","shell.execute_reply":"2022-08-11T04:02:28.857750Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"data_size = len(train_generator) \n#data_size2 = train_generator.n\n\n#print(f\"data_size: {data_size}, {data_size2}\")\n\nsteps_per_epoch = int(data_size / batch_size)\nprint(f\"steps_per_epoch: {steps_per_epoch}\")\n\nval_steps = int(len(val_generator) // batch_size)\n#print(f\"val size: {len(val_generator)}\")\nprint(f\"val_steps: {val_steps}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:34.296532Z","iopub.execute_input":"2022-08-11T04:02:34.297295Z","iopub.status.idle":"2022-08-11T04:02:34.302984Z","shell.execute_reply.started":"2022-08-11T04:02:34.297260Z","shell.execute_reply":"2022-08-11T04:02:34.302155Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Compiling the model\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"categorical_crossentropy\",\n    #metrics=\"accuracy\"\n    metrics=['accuracy', 'Recall', 'Precision', 'AUC']\n\n)\n#model.optimizer.lr=0.001","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:37.591800Z","iopub.execute_input":"2022-08-11T04:02:37.592102Z","iopub.status.idle":"2022-08-11T04:02:37.606539Z","shell.execute_reply.started":"2022-08-11T04:02:37.592079Z","shell.execute_reply":"2022-08-11T04:02:37.605836Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Early Stopping\n\nBefore training the network, we define an early stopping criterion, to avoid redundent epochs once the model has already converged.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:45.348847Z","iopub.execute_input":"2022-08-11T04:02:45.349159Z","iopub.status.idle":"2022-08-11T04:02:45.354848Z","shell.execute_reply.started":"2022-08-11T04:02:45.349135Z","shell.execute_reply":"2022-08-11T04:02:45.353849Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Learning Rate On Plateau\nWe define a `ReduceLROnPlateau` callback to reduce the learning rate when the metric we chose (`val_loss`) has stopped improving.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nlrr = ReduceLROnPlateau(monitor='val_loss',patience=8,verbose=1,factor=0.5, min_lr=0.00001)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:02:51.085940Z","iopub.execute_input":"2022-08-11T04:02:51.086260Z","iopub.status.idle":"2022-08-11T04:02:51.090585Z","shell.execute_reply.started":"2022-08-11T04:02:51.086226Z","shell.execute_reply":"2022-08-11T04:02:51.089662Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Fit the model on train data\n\nmodel_history = model.fit_generator(\n    generator=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    shuffle=True,\n    #verbose=2,\n    validation_data=val_generator,\n    validation_steps=val_steps,\n    callbacks=[early_stopping, lrr]\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:03:14.056224Z","iopub.execute_input":"2022-08-11T04:03:14.056540Z","iopub.status.idle":"2022-08-11T04:03:34.419894Z","shell.execute_reply.started":"2022-08-11T04:03:14.056515Z","shell.execute_reply":"2022-08-11T04:03:34.418380Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Evaluate model performance on test data\nmodel_loss, model_acc, recall, precision, auc = model.evaluate(test_generator)\nprint(\"Model has a loss of %.2f and accuracy %.2f%%\" % (model_loss, model_acc*100))\nprint(\"Model has a recall of %.2f%%, precision of %.2f%% and auc of %.2f%%\" % (recall*100, precision*100, auc*100))","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:04:40.711868Z","iopub.execute_input":"2022-08-11T04:04:40.712195Z","iopub.status.idle":"2022-08-11T04:04:55.765081Z","shell.execute_reply.started":"2022-08-11T04:04:40.712171Z","shell.execute_reply":"2022-08-11T04:04:55.764508Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_generator)\nprint(\"predictions shape:\", predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:05:32.652193Z","iopub.execute_input":"2022-08-11T04:05:32.653345Z","iopub.status.idle":"2022-08-11T04:05:32.962524Z","shell.execute_reply.started":"2022-08-11T04:05:32.653284Z","shell.execute_reply":"2022-08-11T04:05:32.961244Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def plot_loss_and_accuracy(history):\n    history_df = pd.DataFrame(history)\n    fig, ax = plt.subplots(1,2, figsize=(12, 6))\n    \n    history_df.loc[0:, ['loss', 'val_loss']].plot(ax=ax[0])\n    ax[0].set(xlabel = 'epoch number', ylabel = 'loss')\n\n    history_df.loc[0:, ['accuracy', 'val_accuracy']].plot(ax=ax[1])\n    ax[1].set(xlabel = 'epoch number', ylabel = 'accuracy')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:05:39.141081Z","iopub.execute_input":"2022-08-11T04:05:39.141406Z","iopub.status.idle":"2022-08-11T04:05:39.148629Z","shell.execute_reply.started":"2022-08-11T04:05:39.141383Z","shell.execute_reply":"2022-08-11T04:05:39.147306Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"plot_loss_and_accuracy(model_history.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:05:42.238103Z","iopub.execute_input":"2022-08-11T04:05:42.238735Z","iopub.status.idle":"2022-08-11T04:05:42.512222Z","shell.execute_reply.started":"2022-08-11T04:05:42.238699Z","shell.execute_reply":"2022-08-11T04:05:42.511625Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Randomly choose one image from the test set and examine the difference between the original labeling and the model prediction  ","metadata":{}},{"cell_type":"code","source":"import cv2\n\npaths = test_generator.filenames # Your files path\n\ny_pred = model.predict(test_generator).argmax(axis=1) # Predict prob and get Class Indices\nclasses = test_generator.class_indices  # Map of Indices to Class name\n\na_img_rand = np.random.randint(0,len(paths))   # A rand to pick a rand image\nimg = cv2.imread(os.path.join(output_data_path,'test', paths[a_img_rand]))      \ncolored_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # colored output image\n\n#img = image.img_to_array(img)\nplt.imshow(colored_img)\ntrue_label = paths[a_img_rand].split('/')[0]\npredicted_label = list(classes)[y_pred[a_img_rand]]\nprint(f'Class Predicted: {predicted_label} , True label: {true_label}')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:05:49.627200Z","iopub.execute_input":"2022-08-11T04:05:49.627566Z","iopub.status.idle":"2022-08-11T04:05:50.196310Z","shell.execute_reply.started":"2022-08-11T04:05:49.627543Z","shell.execute_reply":"2022-08-11T04:05:50.195533Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score, classification_report\n\nimport seaborn as sns \n\ndef evaluation(y, y_hat, title = 'Confusion Matrix'):\n    \"\"\"Evaluation function\n    Prints Classification reports and confusion matrix\n    \"\"\"\n    cm = confusion_matrix(y, y_hat)\n#     precision = precision_score(y, y_hat)\n#     recall = recall_score(y, y_hat)\n#     accuracy = accuracy_score(y,y_hat)\n#     f1 = f1_score(y,y_hat)\n#     report = classification_report(y,y_hat)\n#     print(report)\n#     print('Recall: ', recall)\n#     print('Accuracy: ', accuracy)\n#     print('Precision: ', precision)\n#     print('F1: ', f1)\n    sns.heatmap(cm,  cmap= 'PuBu', annot=True, fmt='g', annot_kws={'size':20})\n    plt.xlabel('predicted', fontsize=18)\n    plt.ylabel('actual', fontsize=18)\n    plt.title(title, fontsize=18)\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:05:59.183767Z","iopub.execute_input":"2022-08-11T04:05:59.184070Z","iopub.status.idle":"2022-08-11T04:05:59.336738Z","shell.execute_reply.started":"2022-08-11T04:05:59.184047Z","shell.execute_reply":"2022-08-11T04:05:59.335743Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"y_true = test_generator.labels\ny_pred = model.predict(test_generator).argmax(axis=1) # Predict prob and get Class Indices\n\nevaluation(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:06:12.451989Z","iopub.execute_input":"2022-08-11T04:06:12.452300Z","iopub.status.idle":"2022-08-11T04:06:12.858866Z","shell.execute_reply.started":"2022-08-11T04:06:12.452277Z","shell.execute_reply":"2022-08-11T04:06:12.857736Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"classes","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:06:18.955141Z","iopub.execute_input":"2022-08-11T04:06:18.955484Z","iopub.status.idle":"2022-08-11T04:06:18.962127Z","shell.execute_reply.started":"2022-08-11T04:06:18.955461Z","shell.execute_reply":"2022-08-11T04:06:18.961144Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"np.bincount(y_pred) # Use bincount() to count occurrences of each class in a predictions NumPy array","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:06:21.993752Z","iopub.execute_input":"2022-08-11T04:06:21.994084Z","iopub.status.idle":"2022-08-11T04:06:22.000689Z","shell.execute_reply.started":"2022-08-11T04:06:21.994060Z","shell.execute_reply":"2022-08-11T04:06:21.999729Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Testing the Results","metadata":{}},{"cell_type":"code","source":"import cv2\nimg = cv2.imread(os.path.join(input_data_path, f'{final_test_image}.png'))      \ncolored_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # colored output image\n\n#print(type(colored_img))\nmask_label = {0:'MASK INCORRECT',1:'MASK', 2:'NO MASK'}\ncolor_label = {0:(0,255,255),1:(0, 255,0), 2:(255,0,0)}\ncropped_faces = extract_faces(f'{final_test_image}.png', df_final_test)\n\ndf_test_img = df_final_test[['xmin', 'ymin', 'xmax', 'ymax', 'label']]\n#df_test_img\n#for row_num in range(len(df_test_img)):\nfor idx, face in enumerate(cropped_faces):    \n    #print(idx)\n    x_min, y_min, x_max, y_max, label = df_test_img.iloc[idx] \n    #print(x_min, y_min, x_max, y_max, label)\n    #print(face[0])\n    resized_face = cv2.resize(np.array(face[0]),(35,35))\n    reshaped_face = np.reshape(resized_face,[1,35,35,3])/255.0\n\n    face_result = model.predict(reshaped_face)\n    cv2.putText(colored_img,mask_label[face_result.argmax()],(x_min, y_min-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,color_label[face_result.argmax()],2)\n    cv2.rectangle(colored_img,(x_min, y_min), (x_max, y_max), color_label[face_result.argmax()]) # print a blue rectangle of each person's face using the given coordinates\n\nplt.figure(figsize=(10, 10))    \nplt.imshow(colored_img)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:06:27.943232Z","iopub.execute_input":"2022-08-11T04:06:27.943627Z","iopub.status.idle":"2022-08-11T04:06:28.698943Z","shell.execute_reply.started":"2022-08-11T04:06:27.943599Z","shell.execute_reply":"2022-08-11T04:06:28.697607Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Suggestions for improving the model\n\n1. Collect more images of people wearing the mask incorrectly, since although in reality Wearing a mask incorrectly is like not wearing a mask at all, the model classifies it as wearing a mask. \n\n2. Use transfer learning with a pretrained model, such as VGG19 (we must collect more data first).\n","metadata":{}}]}